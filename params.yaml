# TRP Model configuration file

model_config:
  max_len: 64 # max sentence length
  nfeat: 512 # number of features
  nhead: 8 # number of attention heads
  nlayer: 6 # number of attention layers
  nff: 2048 # number of feed forward layer
  dropout: 0.1 # drop ratio
  act_fn: 'relu' # activation function
  beam_size: -1

train_config:
  use_cuda: True
  use_fp16: True
  lr: 1e-4
  optimizer: 'adam'
  opt_level: 'O1'
  checkpoint: 'latest.pt'
  nepochs: 128
  batch_size: 128
  nsave: 8 # save every 8 epochs
  nworkers: 0 # number of workers

data_config:
  vocab_file: "./vocab.pkl"
  train_file: "~/HDD/ChemicalReaction/US_patents_1976-Sep2016_1product_reactions_train.csv"
  valid_file: "~/HDD/ChemicalReaction/US_patents_1976-Sep2016_1product_reactions_valid.csv"
  test_file: "~/HDD/ChemicalReaction/US_patents_1976-Sep2016_1product_reactions_test.csv"